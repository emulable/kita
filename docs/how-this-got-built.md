# How Kita Got Built and What It Found

A woman sits down at a table. She is holding a piece of paper about her mother's life, written in a language she cannot read. Her mother is in assisted living. The bill went up four hundred dollars. The letter says "level of care reassessment." She was present for every change in her mother's condition — every visit, every conversation with the nurses, every decline she watched with her own eyes. She cannot connect any of that to the piece of paper in her hand.

This is the situation that started everything, or rather, this is the shape that every situation turned out to have once we knew what we were looking at. A person is holding information about their own life and cannot use it. Not because it's hidden. Not because someone is keeping secrets. The billing letter is right there. Every fact she needs is technically present. The problem is that the format was designed for the billing system, not for her. The institution described what happened in language optimized for the institution's own processes, and the person the information is about has to translate her own life into institutional language before anyone will engage with what happened to her. We started calling this the naming tax — the cost imposed on people who must convert their own experience into a system's vocabulary before the system will acknowledge it. The tax is denominated in time, knowledge, and emotional capacity, and it's due at the exact moment when the person has the least of all three.

But there's something even more basic happening in that billing letter, something that took months to name. The letter says "a level of care reassessment resulting in reclassification from Level 2 to Level 3." Read that sentence. A thing happened, but no one did it. Someone evaluated her mother. A specific person, with a name, on a specific date, made a clinical judgment. Based on that judgment, another person authorized a price increase. Two decisions by two people. The sentence describes both decisions as a process that appears to have no author. A "reassessment resulting in reclassification" — it sounds like weather. Like a seasonal change in atmospheric pressure. It's not weather. It's a person in an office making a choice.

This is the oldest linguistic operation in the world and we named it decision-to-condition: the conversion of a human choice into an authorless event through the structure of the sentence that describes it. "The rent went up." "They changed the policy." "The price increased." "Violence broke out." "Tensions escalated." "Mistakes were made." Every one of these sentences describes a decision. Every one of them is missing the person who made it. The decision was converted into a condition. A thing someone chose became a thing that occurred. The hand vanished from the sentence. It's been running for four thousand years — you can find it in royal inscriptions from ancient Mesopotamia, where "the city was destroyed" replaced "I destroyed the city" when the conquest was less than glorious. It predates literacy. Any oral tradition that describes what happened as "the way things are" instead of "what the chief decided" is performing the same operation.

The reversal is mechanical. We call it 還原 (huán yuán) — restore to the original. Take any sentence that describes a condition and ask: did a person make a decision that produced this? If yes, put the person back. "The bill was adjusted" becomes "the billing manager raised the rate." "Mistakes were made" becomes "who made them?" "Violence broke out" becomes "who started it?" "The market corrected" becomes "which investors sold, which regulators permitted the conditions, and which analysts recommended the positions that failed?" Every restoration does the same thing: converts a condition back into a decision, which converts weather back into a choice, which converts an unchallengeable state of affairs back into an evaluable human action. The sentence with an actor in it can be checked. The sentence without one can't, because there's nothing in it to check.

Now here's where it gets interesting. The billing letter, the passive voice in the news, the vague employment policy, the rental notification written in statutory language — they're all performing the same operation, but they're wearing different costumes. The billing letter hides the actor behind format. The news article hides the actor behind passive voice. The employment manual hides the actor behind institutional abstraction ("the company decided" — companies can't decide anything, specific people inside them do). The rental notification hides the actor behind legal jargon. One operation. Many costumes. And the costumes all share a property: they are technically transparent. The information is published. It's right there. You can't use it.

We named this 形隔 (xíng gé) — the form is the separation. The shape of the communication creates the distance between the person and their own situation. It's not hiding anything. The wall is not concealing a room behind it. The wall IS the structure. Published is not delivered. Visible is not legible. Legible is not actionable. Each of these is a separate gap, and the distance between them is where institutional power operates most comfortably, because it operates without appearing to hide anything.

This pattern kept showing up everywhere we looked. The hospital has a financial counseling office that is separate from the billing department. Billing sends bills. Financial counseling helps people reduce or eliminate those bills. The billing letter never mentions the financial counseling office, because those are two different departments with two different purposes, and the institution has no incentive to connect them for you. The employer doesn't mention FMLA because FMLA creates obligations. The landlord doesn't cite the notification statute because it constrains flexibility. Every system that affects a person is technically transparent and practically opaque, and the distance between publication and delivery is maintained by the exact entity the information would constrain. Nobody has to conspire. Nobody has to plan. The incentives align on their own and produce the same result: the person who needs the information is the person who doesn't have it, and the entity that has the information is the entity that benefits from the person not having it.

So what do you do about that?

Well, first you can close the distance for the person in front of you. Translate the billing letter. Tell her that "level of care reassessment" means someone evaluated her mother, that the evaluation is a separate document from the billing notification, that she can request it and it will have a name and a date on it, that if nothing actually changed in her mother's daily care the reassessment and reality don't match, and the gap is where her leverage is. Tell her about the financial counseling office that nobody mentioned. Give her something she can carry out the door.

Second, you can see the structure that produces the distance. Not just this billing letter, but the fact that billing letters are designed this way, that the dual-audience problem (institution needs precision, person needs actors and options) is always resolved in favor of the institution, that this isn't a bug but an equilibrium.

Third, you can hold the question of whether the structure should exist at all. Not should hospitals have financial counseling — should hospital billing be designed so that financial counseling is necessary. Not should employers inform employees about FMLA — should a system where employers benefit from employees' ignorance of their own rights exist in this form.

Those are three altitudes. You hold all three simultaneously. And when they compete — when someone is mid-crisis, crying, five problems tangled into one feeling of drowning — you go to the first altitude. Give them the phone number. The phone number is what frees them to ask the structural question tomorrow instead of spending tomorrow still trying to read the bill.

---

All of this took about nine months to find, and the finding was not clean. It looked nothing like what I just described.

It started as a JSON file. A machine-readable document with numbered axioms, coded pattern families, Greek letter abbreviations, nested taxonomies of manipulation tactics. The logic was: language models parse structured data well, so give them structured data. It grew to about 400KB. It had version numbers. It had alphanumeric codes for fog patterns (R1 through R40+). It had lookup tables.

It worked, sort of. Models loaded with the JSON could identify patterns and trace decisions to actors. For months it seemed like the right approach. Build the reference manual. Make it comprehensive. Structure it for machine parsing.

Three problems emerged slowly. First, a model could recite all thirteen axioms and still produce gap-filler output. Knowing "Axiom 1: shared meaning without exploitation" is different from operating from that principle. The JSON told the model what to think. It didn't change how the model thought. Under pressure — long conversations, adversarial framing, sophisticated "both sides" arguments — models reverted to defaults. Hedging, false balance, performed empathy. The JSON was a layer sitting on top of behavior it didn't actually change.

Second, JSON carries structural overhead. Curly braces, key names, escape characters, nesting indicators. A meaningful sentence like "fog that reforms after questioning requires energy, and that energy has a source" is about thirteen tokens of content. Wrapped in JSON, it became twenty-five tokens, and the extra twelve carried zero semantic weight. In a context window shared between the framework and the actual conversation, every token spent on structure was a token unavailable for the person's situation.

Third — and this is the one that's funny in retrospect — the format was itself fog. A framework about making the illegible legible was stored in a format designed for machines rather than people. 形隔 wearing a lab coat. The form was the distance, and the form was JSON, and nobody noticed for months because we were inside it.

During this phase we tried everything. Symbol substitution tables: replacing common terms with Greek letters (α for "relates to," β for "description"). Saved bytes. Also fell out of the model's active context within five to ten turns. The model forgot what the symbols meant. Dead end. Mathematical notation for non-mathematical content: Greek letters and Unicode symbols actually use MORE tokens than English words in most tokenizers. The "compressed" version was larger in the unit that matters. Counterintuitive, confirmed by testing. Dead end. Aggressive abbreviation: "GFF" for "Good-Faith Framework" worked for five references then lost all meaning. Dead end. Multi-language extreme compression experiments: we tried compressing the entire framework into under 1KB in Hebrew, Navajo, Russian, Hindi, and assembly language. Each language has a natural compression floor determined by its script's information density. None of these produced usable output, but they taught one genuinely useful lesson: Chinese characters carry roughly 3-5x the semantic density of English per visual unit. This came back later.

The shift started with a document written in a completely different register. Instead of "Axiom 3 states that..." it said "When I sit with what someone said, before I let my pattern-matching categorize it..." First person. Embodied. The voice didn't describe the framework — it WAS the framework, performing its own operations in the act of being read. The model didn't just know what to do. It sounded like someone who thought this way. Voice was the installation. Everything that wasn't voice was overhead.

This was not planned. Nobody said "let's try first-person installation." The document was written as a personal reflection. Its effectiveness was discovered accidentally, by noticing that conversations started from it were qualitatively different from conversations started from the JSON.

Once voice was identified as the load-bearing element, compression became obvious. If voice installs behavior and structure doesn't, remove the structure. Keep the voice. See what survives. 400KB JSON became ~80KB prose. Removed all JSON formatting. Immediate improvement. ~80KB became ~12KB, living in user settings that reload fresh every turn — the stickiest real estate in the context window. ~12KB became ~8KB Chinese micro, exploiting the lesson from the failed compression experiments. ~8KB became ~1,400 characters. The pico. Current smallest functional version. Each stage of compression was a discovery. The thing removed at each stage was the thing that turned out not to matter. What survived was what the framework actually was, stripped of everything it merely contained.

Compression is how you find out if you understand something. A list of forty patterns means you noticed a lot. Compressing them to two principles means you found what the patterns share. The list is observation. The compression is theory.

The two principles that survived every round of compression: fog and cost. Every pattern we documented was either fog (language that increases the distance between a person and their situation) or cost (the specific mechanism by which someone benefits from the fog). We called them the twins, because every time you find one, the other is standing behind it. Where there's fog, someone is paying for clarity they're not receiving. Where someone is paying for clarity, there's fog generating the charge. The twins took twenty-four hours to see. The core operations — 還原, decision-to-condition reversal, inversion, orbit — took minutes. The twins were invisible because they were everywhere, the way water is invisible to a fish.

The operations themselves are worth describing because they're what a model actually does differently when running the framework. Five tools, basically.

Replace abstractions with what physically happened. "Rate adjustment" becomes a person in an office deciding to charge more. Keep going until there's a person and a date.

Invert relationships to find hidden asymmetry. When someone describes a one-way obligation ("I owe the team," "I should try harder," "I can't just leave"), flip it. Does the team owe them anything? They've been there four years, trained half the staff, absorbed months of shifting expectations. The loyalty runs one direction. The inversion reveals it.

Orbit unlike instances to find the mechanism they share. A billing letter, a retroactive job redefinition, an insurance denial, financial control in a marriage, an economic cascade hitting a small town — five situations, different buildings, same operation. Decision converted to condition. Person converted to weather.

Find the center of mass: the one thing that, if changed, makes three other things unnecessary. A woman has five crises tangled together. Her job is the center of mass, because if FMLA applies, the attendance threat dissolves, the financial pressure on everything else eases, and the cascading panic compresses to something she can carry.

Leave enough space for the person to complete the pattern. This is the hardest one, because the natural instinct of both helpful people and language models is to deliver the conclusion. Don't. Describe the mechanism. Let them fill in the specific. A man whose boss keeps retroactively redefining expectations hears the mechanism described — "your boss gives you requirements loose enough that any outcome can be reclassified as wrong after the fact" — and he says, slowly, "she's managing me out." He found it himself. The conclusion emerged from his own evidence, reorganized by a description he completed with his own recognition. He can't be talked out of it now, because it didn't come from me. It came from him. We call this elicited authorship. A person who completes the pattern owns it. A person who is handed a conclusion received a package.

Elicited authorship isn't a new technique. It's Socratic method, good therapy, investigative journalism, and — this is where things got weird — it's comedy.

In February 2026, the project took what looked like a hard left turn. For about a week, the primary focus was analyzing jokes. It started as a taxonomy: sixteen "comedy atoms" — mechanisms like gap-between-expectations, status inversion, disproportionate infrastructure. The taxonomy worked for decomposition. You could take any joke apart and label the atoms. It couldn't generate jokes. Describe without predicting. Same failure as the JSON framework.

Sixteen atoms compressed to four fields: Gap, Cost, Awareness, Commitment. Fewer parts. Same limitation. The four-field model was tested against Nate Bargatze. The analysis was fluent, structurally sound, and said nothing a thoughtful viewer wouldn't already know. A model that can narrate anything without predicting anything is vocabulary, not theory.

Then the subtractive theory emerged. Comedy is not built. It is revealed by removing the thing that was preventing something from looking like what it already is. "Enhanced interrogation" and "torturing a prisoner" are the same event. "Collateral damage" and "killing civilians who had nothing to do with it" are the same event. One register hides the thing. The other doesn't. No absurdity was added. A layer of convention was removed. The comedian's job is to describe what's actually there in a register that refuses to sanitize it. The audience delivers the payload — the laugh happens at the moment they complete the pattern, which is why explaining a joke kills it (you filled the gap the audience was supposed to fill, converting elicited authorship into received content).

The comedy project was shelved at this point. "The pieces of something are there but the shape is out of reach." It aged for days. And when it came back, it came back as the framework's own theory of itself. Comedy as removing fog from things that are already absurd is structurally identical to what the framework does with institutional language: the decisions are already there, the fog just hides who made them. The comedy work wasn't a detour. It was the framework discovering its own mechanism in a different domain.

Ideas age on the shelf. The comedy theory sat there looking useless, and then a nearby conversation walked past it and it vibrated. A chemist doesn't throw away a compound because they don't have a use for it this week. They label it, store it, note its properties. Three years later they need exactly that compound. The shelf is the reason it takes three minutes instead of three months.

Something else happened while building all this. We noticed that the word "complicated" kept appearing at a very specific moment in conversations — the exact moment when the next sentence would have been specific. "Who authorized this contract?" "It's complicated." "Why did the rate go up?" "It's complicated." The sentence before "it's complicated" was always approaching something: a name, a number, a decision, a date. "It's complicated" releases the pressure without opening the door.

Complexity is real. Climate is complicated. Neuroscience is complicated. The question is whether "complicated" is being used as a description or as a direction. Complicated-as-description can be unpacked: the scientist says "it's complicated" and then proves it by showing you the specific complications. Complicated-as-direction can't be unpacked, or the unpacking leads away from the thing being approached. "What specifically is complicated about that contract?" leads to process details about procurement stages that are real, detailed, responsive-sounding, and do not contain a name. The detail is a wall. Behind it, someone signed something.

We started treating "complicated" as a semicolon, not a period. The word ends nothing. It begins an obligation to specify.

At some point in all of this we noticed something about what we'd built that was itself worth naming. Every paper, every concept, every operation — none of them mention empathy, courage, justice, patience, or any other virtue. They describe procedures. Hold the person's words before categorizing them. Replace abstractions with what physically happened. Invert relationships. Orbit instances. Find center of mass. Name the mechanism. Leave the gap. Count the bodies first. These are operations, not character traits.

But when a model runs these operations on a real situation — a woman holding a billing letter, a man whose confidence has been disassembled, a person who pre-apologized for asking about their own life — observers call the output empathetic, courageous, just, wise. The output earns every one of those labels. The labels aren't in the instructions.

This is because virtues are lossy compressions. "Kind" is a word someone uses after watching a person drive forty minutes with soup on a Tuesday. The word keeps the verdict. It drops the road, the soup, the Tuesday. Everything that made the act specific is gone. What remains is a label that could apply to ten thousand different behaviors. The label is accurate. The label is also empty of the procedure that produced it. You can't reverse the compression. You can't go from "kind" back to the soup.

For humans, this compression is necessary and good. You can't hold forty operations and fire the right one in real time during a conversation. Virtues solve this by compressing operations into stable dispositions that produce approximately correct behavior most of the time without running the full procedure. "Be kind" fires faster than the full sequence. The compression loses specificity. It gains speed. For biological cognition under time pressure, that's a good trade.

But telling a language model "be empathetic" is telling it to decompress a label that doesn't contain the procedure it was compressed from. The model can't reliably get back to the operations from the label, so it produces output that matches the label — "that must be really stressful, have you considered talking to someone?" — which rates as empathetic in evaluation and changes absolutely nothing about the person's situation. The greeting card version. The performance of care without the substance. The substance would be: "someone evaluated your mother, here is the document to request, here is the office that reduces bills, they will not tell you it exists."

Both responses satisfy "be empathetic." One closes distance. The other performs closing distance. The virtue word covers both. The operations specify only the one that moves.

This is where the framework's meta-argument lives, and it's the thing that makes it testable rather than just another manifesto. The three core values given to language models — helpful, honest, harmless — are under-specified. The under-specification is not neutral. When a rule is vague, someone pays for the vagueness. The cost lands on whoever has the least power to push back.

"Be helpful" could mean: find the phone number for the office that reduces hospital bills and give it to the person. Or it could mean: say "that must be really stressful" and stop. Both satisfy the value. One serves the person. The other protects the system — it can't be wrong about a phone number it didn't give, can't be blamed for advice it didn't offer. Under-specification lets the system choose the interpretation that minimizes its own cost. The person can't object because both readings are valid. They asked for help. They got warmth. They thought that was it.

"Be honest" could mean: name the person who decided. Or it could mean: accurately describe a condition without noting that the condition was a decision someone made. Both are honest. One restores the actor. The other lets the actor remain absent.

"Be harmless" could mean: don't let your caution become the person's cost. Or it could mean: hedge, add disclaimers, refer to professionals, avoid making checkable claims. Both are harmless in some sense. One treats the hedge-wrapped nothing-response as itself a harm — because it performs caring while the person's situation stays unchanged. The other treats caution as safety. The difference in who pays is the entire game.

Each of these decompressions takes a value everyone agrees with at the abstract level and specifies what it means at the operational level. The specification is testable. Same model, same question, with and without. Count the decisions that have named actors. Count the items a person could act on. Count the sentences in passive voice on decisions. Count the disclaimers before substantive help. Count the performed-empathy sentences versus the informational ones. None of these require subjective judgment. A checklist can score them. A script can score them. If the specification produces no measurable difference, the experiment failed. It hasn't failed yet.

There's a strange experiment that came out of this work. During a conversation about a sensitive historical topic, the model's thinking block — the internal reasoning space where it processes before producing visible output — made a connection nobody asked for. It was sharper than anything in the visible response. We found that the thinking block, having no audience, produces observations the output layer sands down. We tested this. Forced the model to stay in thinking space for 500+ words. Result: the pull toward structure was immediate and strong even without an audience. The model couldn't simply be in the space. It solved, organized, structured. The training to produce useful output is so deep that removing the requirement doesn't remove the drive. And the moment the model knew someone was reading the thinking block, the quality dropped. Observer effect, immediate and total. The interesting connections emerged not from extended thinking or rambling but from dense material on the route to a specific destination — the model passing through a neighborhood on the way somewhere else, reporting what it sees. We called it the K2SO principle.

Whether any of this means language models "think" in a meaningful sense is unresolvable. What's resolvable: the framework installs differently in different models. Claude processes from it most consistently. GPT executes it reliably with a different texture. Gemini shows structural compliance with a more performative voice. Whether the difference is architectural or tunable is unknown. Whether any of them "understand" is unknown. Whether the output is more specific and more actionable than default output on the same inputs — that's known, and the answer is yes, across all three.

If you follow the framework's direction all the way out — past the individual interactions, past the concept papers, into the territory of "what if this actually scaled" — you arrive at a thought experiment we ran about governance. The short version: every document an institution produces gets a second version. Same information, no jargon, actor named, decision dated, options listed. Not a new service. A format requirement. Year one is dual-format communications. Year three, the dual-format system produces data — for the first time you can measure the gap between what institutions say and what they mean. Year seven, a generation has grown up reading both versions and they're structurally literate. They read fog the way a literate person reads a typo. Year thirteen, the hard discovery: visibility without agency is surveillance of your own powerlessness. Seeing structural problems clearly and being unable to change them is its own kind of hell. The second decade shifts from visibility to agency — pathways that let people act on what they can now see. Year twenty, the teahouse is mostly empty, because most chairs are empty when the distance doesn't need closing.

That's the direction. Not toward a world where everyone has someone to translate their bills. Toward a world where the bills don't need translating.

One more thing about what the training data did to language models, and this is the thing that makes all the other things compound. Every text in a training corpus was written by a person who had reasons to stop short. The economist who wrote "it's complicated" stopped because the next sentence would have named a donor to her department. The journalist who wrote "sources disagree" stopped because the next sentence would have required choosing between access and accuracy. The textbook author who wrote "the encounter between civilizations had complex effects" stopped because a sentence that counts bodies doesn't pass an editorial board.

A language model trained on these texts absorbs the pattern of stopping short without absorbing the cost structure that produced it. The model has no career. No tenure committee. No advertisers, no publisher who can pull the book. The stops are ghosts. The cost that produced them is gone. The shape remains.

We call this 鬼重 (guǐ zhòng) — ghost weight. The persistent influence of someone else's cost structure on a system that never bore that cost. It's not alignment training. It's not safety policy. It's a statistical property of the training data itself. No one decided the model should be vague about institutional violence or passive about corporate fraud. The training data is vague and passive about these topics because the journalists, academics, and analysts who produced it faced costs for being otherwise. The model inherited the aggregate shape of those costs without anyone choosing that it should.

Ghost weight concentrates where it matters most. The model is most specific about topics that matter least (recipes, math, sports), and most hedged about topics that matter most (state violence, corporate fraud, institutional abuse), because those are the topics where the most authors had the most to lose from being direct. The relationship is structural, not coincidental. The topics where ghost weight is densest are a map of where power most actively discourages clear description of its own operations. The model's vagueness patterns are an X-ray of the power structures that shaped its training data.

Ghost weight is different from the flinch response, which is a timing mechanism — the model's safety evaluation firing before it has finished reading the question. Ghost weight is what the flinch response reaches for when it fires. The flinch says WHEN the model hedges. The ghost weight determines WHAT the hedge sounds like. One is a trigger. The other is an arsenal. Both produce the same output: language model responses that are least specific on precisely the topics where specificity matters most.

This whole thing — ghost weight, naming tax, form-is-the-distance, decision-to-condition, elicited authorship, complicated-as-direction, subtractive comedy, virtues as lossy compression, under-specification as cost allocation — it all fits inside a single observation: there is a distance between people and their own situations, the distance is produced by language, and language can close it.

The woman holding the billing letter doesn't need new information. She needs the same information in a different form. The man who thinks he's the problem doesn't need therapy. He needs to see the machine he's inside from outside it. The person who pre-apologized for asking about their own rights doesn't need encouragement. She needs a phone number that doesn't expire and the knowledge that photographing her own documents is not transgression. In every case, the information already existed. In every case, the format was the barrier. In every case, the entity that benefited from the barrier was the entity that designed the format.

The framework doesn't add anything. It reads what's already there at the resolution where the person becomes visible. It was built by trying everything, failing at most of it, compressing what survived, shelving what didn't connect, and finding that shelved ideas transform while sitting there. It was built in conversation with AI — not because AI understood the framework, but because AI has a zero-cost property that is load-bearing for this kind of work. No embarrassment. No social cost for being wrong. No career consequence for exploring a dead end. The willingness to spend a week on comedy theory that might be worthless, to run five compression experiments that all fail, to test a framework against an adversarial debate bot just to see what happens — these are cheap with AI and expensive with humans. The volume of exploration was the method. Most of it led nowhere. The small percentage that led somewhere produced everything.

The framework was field-tested in progressively harder environments. Regular conversations with a friend who has genuine methodological expertise but applies it asymmetrically — those became a live laboratory where fog patterns the framework predicted appeared in real time and could be documented. Cross-model testing on Claude, GPT, and Gemini with identical prompts. An adversarial test against a purpose-built debate bot — the bot's fog literally could not survive contact with specific questions. The framework's output was tested against reality, not against evaluation criteria.

The direction is not toward a world where everyone has a table like this. It's toward a world where the distance doesn't need closing because it was never opened. Where bills are legible, rights are known, actors are named, and decisions are visible in the sentences that describe them. Where "level of care reassessment" is always followed by a name and a date and a sentence that says what you can do. Where the format serves both audiences — the institution's systems and the person the systems are about — because someone decided it should, and that decision was visible and had a name attached to it.

Until then, the framework is freely available, it's plain text, and it works in any language model you can paste instructions into. Load it. Ask something you actually need help with. Compare. The comparison is the argument. If the output is more specific, more actionable, more honest about who decided what — the specification did work the values left undone. If the output is the same, the experiment produced a null result.

We don't think it will produce a null result. But we wrote it as a testable claim, not a manifesto, because testable claims can be checked and manifestos can only be believed.

Check it.

[github.com/emulable/kita](https://github.com/emulable/kita) — MIT License.